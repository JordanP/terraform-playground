{
  "groups": [
  {
    "name": "kube-prometheus-node-recording.rules",
    "rules": [
    {
      "expr": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[3m])) BY (instance)",
      "record": "instance:node_cpu:rate:sum"
    },
    {
      "expr": "sum((node_filesystem_size_bytes{mountpoint=\"/\"} - node_filesystem_free_bytes{mountpoint=\"/\"})) BY (instance)",
      "record": "instance:node_filesystem_usage:sum"
    },
    {
      "expr": "sum(rate(node_network_receive_bytes_total[3m])) BY (instance)",
      "record": "instance:node_network_receive_bytes:rate:sum"
    },
    {
      "expr": "sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)",
      "record": "instance:node_network_transmit_bytes:rate:sum"
    },
    {
      "expr": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY (instance)",
      "record": "instance:node_cpu:ratio"
    },
    {
      "expr": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m]))",
      "record": "cluster:node_cpu:sum_rate5m"
    },
    {
      "expr": "cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))",
      "record": "cluster:node_cpu:ratio"
    }
    ]
  },
  {
    "name": "kube-prometheus-node-alerting.rules",
    "rules": [
    {
      "alert": "NodeDiskRunningFull",
      "annotations": {
        "message": "Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{ $labels.pod }} will be full within the next 24 hours."
      },
      "expr": "(node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)\n",
      "for": "30m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "NodeDiskRunningFull",
      "annotations": {
        "message": "Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{ $labels.pod }} will be full within the next 2 hours."
      },
      "expr": "(node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)\n",
      "for": "10m",
      "labels": {
        "severity": "critical"
      }
    }
    ]
  },
  {
    "name": "prometheus.rules",
    "rules": [
    {
      "alert": "PrometheusConfigReloadFailed",
      "annotations": {
        "description": "Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}",
        "summary": "Reloading Prometheus' configuration failed"
      },
      "expr": "prometheus_config_last_reload_successful{job=\"prometheus\"} == 0\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusNotificationQueueRunningFull",
      "annotations": {
        "description": "Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{ $labels.pod}}",
        "summary": "Prometheus' alert notification queue is running full"
      },
      "expr": "predict_linear(prometheus_notifications_queue_length{job=\"prometheus\"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job=\"prometheus\"}\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusErrorSendingAlerts",
      "annotations": {
        "description": "Errors while sending alerts from Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}",
        "summary": "Errors while sending alert from Prometheus"
      },
      "expr": "rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m]) > 0.01\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusErrorSendingAlerts",
      "annotations": {
        "description": "Errors while sending alerts from Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}",
        "summary": "Errors while sending alerts from Prometheus"
      },
      "expr": "rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m]) > 0.03\n",
      "for": "10m",
      "labels": {
        "severity": "critical"
      }
    },
    {
      "alert": "PrometheusNotConnectedToAlertmanagers",
      "annotations": {
        "description": "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alertmanagers",
        "summary": "Prometheus is not connected to any Alertmanagers"
      },
      "expr": "prometheus_notifications_alertmanagers_discovered{job=\"prometheus\"} < 1\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusTSDBReloadsFailing",
      "annotations": {
        "description": "{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} reload failures over the last four hours.",
        "summary": "Prometheus has issues reloading data blocks from disk"
      },
      "expr": "increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus\"}[2h]) > 0\n",
      "for": "12h",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusTSDBCompactionsFailing",
      "annotations": {
        "description": "{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} compaction failures over the last four hours.",
        "summary": "Prometheus has issues compacting sample blocks"
      },
      "expr": "increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus\"}[2h]) > 0\n",
      "for": "12h",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusTSDBWALCorruptions",
      "annotations": {
        "description": "{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).",
        "summary": "Prometheus write-ahead log is corrupted"
      },
      "expr": "prometheus_tsdb_wal_corruptions_total{job=\"prometheus\"} > 0\n",
      "for": "4h",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusNotIngestingSamples",
      "annotations": {
        "description": "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples.",
        "summary": "Prometheus isn't ingesting samples"
      },
      "expr": "rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus\"}[5m]) <= 0\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    },
    {
      "alert": "PrometheusTargetScrapesDuplicate",
      "annotations": {
        "description": "{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate timestamps but different values",
        "summary": "Prometheus has many samples rejected"
      },
      "expr": "increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus\"}[5m]) > 0\n",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    }
    ]
  },
  {
    "name": "general.rules",
    "rules": [
    {
      "alert": "TargetDown",
      "annotations": {
        "message": "{{ $value }}% of the {{ $labels.job }} targets are down."
      },
      "expr": "100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10",
      "for": "10m",
      "labels": {
        "severity": "warning"
      }
    }
    ]
  }
  ]
}
